{
   "source": [
    "## Кластеризация\n",
    "\n",
    "Интуитивная постановка задачи кластеризации довольно проста и представляет из себя наше желание сказать: \"Вот тут у меня насыпаны точки. Я вижу, что они сваливаются в какие-то кучки вместе. Было бы круто иметь возможность эти точки относить к кучкам и в случае появления новой точки на плоскости говорить, в какую кучку она падает.\" Из такой постановки видно, что пространства для фантазии получается много, и от этого возникает соответствующее множество алгоритмов решения этой задачи. Перечисленные алгоритмы ни в коем случае не описывают данное множество полностью, но являются примерами самых популярных методов решения задачи кластеризации.\n",
    "\n",
    "<figure><img align=\"center\" src=\"https://habrastorage.org/getpro/habr/post_images/8b9/ae5/586/8b9ae55861f22a2809e8b3a00ef815ad.png\"><figcaption>Примеры работы алгоритмов кластеризации из документации пакета scikit-learn</figcaption></figure>\n",
    "\n",
    "### K-means\n",
    "\n",
    "Алгоритм К-средних, наверное, самый популярный и простой алгоритм кластеризации и очень легко представляется в виде простого псевдокода:\n",
    "1. Выбрать количество кластеров $inline$k$inline$, которое нам кажется оптимальным для наших данных.\n",
    "2. Высыпать случайным образом в пространство наших данных $inline$k$inline$ точек (центроидов).\n",
    "3. Для каждой точки нашего набора данных посчитать, к какому центроиду она ближе.\n",
    "4. Переместить каждый центроид в центр выборки, которую мы отнесли к этому центроиду.\n",
    "5. Повторять последние два шага фиксированное число раз, либо до тех пор пока центроиды не \"сойдутся\" (обычно это значит, что их смещение относительно предыдущего положения не превышает какого-то заранее заданного небольшого значения).\n",
    "\n",
    "В случае обычной евклидовой метрики для точек лежащих на плоскости, этот алгоритм очень просто расписывается аналитически и рисуется. Давайте посмотрим соответствующий пример:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Начнём с того, что насыпем на плоскость три кластера точек\n",
    "X = np.zeros((150, 2))\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "X[:50, 0] = np.random.normal(loc=0.0, scale=0.3, size=50)\n",
    "X[:50, 1] = np.random.normal(loc=0.0, scale=0.3, size=50)\n",
    "\n",
    "X[50:100, 0] = np.random.normal(loc=2.0, scale=0.5, size=50)\n",
    "X[50:100, 1] = np.random.normal(loc=-1.0, scale=0.2, size=50)\n",
    "\n",
    "X[100:150, 0] = np.random.normal(loc=-1.0, scale=0.2, size=50)\n",
    "X[100:150, 1] = np.random.normal(loc=2.0, scale=0.5, size=50)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(X[:, 0], X[:, 1], \"bo\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В scipy есть замечательная функция, которая считает расстояния\n",
    "# между парами точек из двух массивов, подающихся ей на вход\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Прибьём рандомность и насыпем три случайные центроиды для начала\n",
    "np.random.seed(seed=42)\n",
    "centroids = np.random.normal(loc=0.0, scale=1.0, size=6)\n",
    "centroids = centroids.reshape((3, 2))\n",
    "\n",
    "cent_history = []\n",
    "cent_history.append(centroids)\n",
    "\n",
    "for i in range(3):\n",
    "    # Считаем расстояния от наблюдений до центроид\n",
    "    distances = cdist(X, centroids)\n",
    "    # Смотрим, до какой центроиде каждой точке ближе всего\n",
    "    labels = distances.argmin(axis=1)\n",
    "\n",
    "    # Положим в каждую новую центроиду геометрический центр её точек\n",
    "    centroids = centroids.copy()\n",
    "    centroids[0, :] = np.mean(X[labels == 0, :], axis=0)\n",
    "    centroids[1, :] = np.mean(X[labels == 1, :], axis=0)\n",
    "    centroids[2, :] = np.mean(X[labels == 2, :], axis=0)\n",
    "\n",
    "    cent_history.append(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А теперь нарисуем всю эту красоту\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(4):\n",
    "    distances = cdist(X, cent_history[i])\n",
    "    labels = distances.argmin(axis=1)\n",
    "\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(X[labels == 0, 0], X[labels == 0, 1], \"bo\", label=\"cluster #1\")\n",
    "    plt.plot(X[labels == 1, 0], X[labels == 1, 1], \"co\", label=\"cluster #2\")\n",
    "    plt.plot(X[labels == 2, 0], X[labels == 2, 1], \"mo\", label=\"cluster #3\")\n",
    "    plt.plot(cent_history[i][:, 0], cent_history[i][:, 1], \"rX\")\n",
    "    plt.legend(loc=0)\n",
    "    plt.title(\"Step {:}\".format(i + 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также стоит заметить, что хоть мы и рассматривали евклидово расстояние, алгоритм будет сходиться и в случае любой другой метрики, поэтому для различных задач кластеризации в зависимости от данных можно экспериментировать не только с количеством шагов или критерием сходимости, но и с метрикой, по которой мы считаем расстояния между точками и центроидами кластеров.\n",
    "\n",
    "Другой особенностью этого алгоритма является то, что он чувствителен к исходному положению центроид кластеров в пространстве. В такой ситуации спасает несколько последовательных запусков алгоритма с последующим усреднением полученных кластеров."
   ]
  }
 "nbformat": 4,
 "nbformat_minor": 2
}
